# rag-pdf-chatbot
ğŸ”¥ Project Description
This project implements a fully offline Retrieval-Augmented Generation (RAG) system that allows users to chat with PDF documents without using any paid APIs or cloud services.
The system loads a PDF file, splits it into chunks, converts the text into vector embeddings using HuggingFace models, stores them in ChromaDB, and retrieves relevant information to generate context-aware responses using a local LLM powered by Ollama.

ğŸ§  Key Features
   ğŸ“„ PDF Document Loader
   âœ‚ Text Chunking & Splitting
   ğŸ§  Local HuggingFace Embeddings
   ğŸ—„ Chroma Vector Database
   ğŸ” Semantic Search & Retrieval
   ğŸ¤– Local LLM (Ollama - Llama3 / Phi / TinyLlama)
   ğŸ’» Fully Offline (No API Keys Required)
   ğŸ’° 100% Free Setup

âš™ï¸ Tech Stack
     Python
     LangChain (0.2.x architecture)
     ChromaDB
     HuggingFace Sentence Transformers
     Ollama (Local LLM)

ğŸ”„ How It Works
    PDF â†’ Text Splitting â†’ Embeddings â†’ ChromaDB â†’ Retriever â†’ LLM â†’ Answer

ğŸ¯ Use Cases
    Document Question Answering
    Resume Chatbot
    Research Paper Assistant
    Knowledge Base Chatbot
    Offline AI Applications
